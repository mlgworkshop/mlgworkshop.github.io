<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
        content="MLG 2023, 19th International Workshop on Mining and Learning with Graphs, co-located with KDD 2023, Long Beach, CA, USA">
    <meta name="author" content="Shobeir Fakhraei">

    <title>MLG 2023 - 19th International Workshop on Mining and Learning with Graphs</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/agency.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet'
        type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <style type="text/css">
        .tg {
            border-collapse: collapse;
            border-spacing: 0;
            width: 520px
        }

        .tg td {
            font-family: Arial, sans-serif;
            font-size: 14px;
            padding: 12px 12px;
            border-style: solid;
            border-width: 0px;
            overflow: hidden;
            word-break: normal;
            border-top-width: 1px;
            border-bottom-width: 1px;
        }

        .tg th {
            font-family: Arial, sans-serif;
            font-size: 14px;
            font-weight: normal;
            padding: 12px 12px;
            border-style: solid;
            border-width: 0px;
            overflow: hidden;
            word-break: normal;
            border-top-width: 1px;
            border-bottom-width: 1px;
        }

        .tg .tg-lqy6 {
            text-align: right;
            vertical-align: top;
            width: 100px
        }

        .tg .tg-odj0 {
            font-weight: bold;
            background-color: #ffcb2f;
            vertical-align: top
        }

        .tg .tg-yw4l {
            vertical-align: top
        }

        .tg .tg-l2oz {
            font-weight: bold;
            text-align: right;
            vertical-align: top
        }

        .tg .tg-9hbo {
            font-weight: bold;
            vertical-align: top
        }

        .tg .tg-xr8r {
            background-color: #ffffc7;
            text-align: right;
            vertical-align: top;
            width: 100px
        }

        .tg .tg-kjho {
            background-color: #ffffc7;
            vertical-align: top
        }
    </style>


</head>

<body id="page-top" class="index">
    <!-- Google Analytics -->
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-75238067-1', 'auto');
        ga('require', 'linkid');
        ga('send', 'pageview');

    </script>

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse"
                    data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top"><img src="img/mlg-logo.gif"
                        style="margin:0px; padding:0px; height:30px" /></a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#introduction">Intro</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#program">Schedule</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#keynote">Keynotes</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#papers">Accepted Papers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#call">CFP</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#dates">Dates</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#organization">Organization</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#history">History</a>
                    </li>
                    <li>
                        <a href="https://twitter.com/mlgworkshop" target=_blank><i class="fa fa-twitter"
                                style="font-size:20px;"></i></a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <!-- Header -->
    <header>
        <div class="container">
            <div class="intro-text">
                <div class="intro-lead-in">Held in conjunction with <a href="http://www.kdd.org/kdd2023/"
                        target=_blank>KDD'23</a></br>
                    Monday, August 7, 2023 - Long Beach CA, USA</div>
                <div class="intro-heading">19th International Workshop on<br />Mining and Learning with Graphs</div>
                <a href="#program" class="page-scroll btn btn-xl">Program Schedule</a>
            </div>
        </div>
    </header>

    <!-- Introduction Section -->
    <section id="introduction">
        <!--class="bg-mid-gray"-->
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Introduction</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row text-justify">

                <div class="row text-justify">

                    <div class="col-md-12">
                        <p class="large text-muted">
                            There is a great deal of interest in analyzing data that is best represented as a graph.
                            Examples include the WWW, social networks, biological networks, communication networks,
                            transportation networks, energy grids, and many others. These graphs are typically
                            multi-modal, multi-relational and dynamic. In the era of big data, the importance of being
                            able to effectively mine and learn from such data is growing, as more and more structured
                            and semi-structured data is becoming available. The workshop serves as a forum for
                            researchers from a variety of fields working on mining and learning from graphs to share and
                            discuss their latest findings.
                            <br />
                            There are many challenges involved in effectively mining and learning from this kind of
                            data, including:
                        </p>
                        <ul class="large text-muted">
                            <li>Understanding the different techniques applicable, including graph mining algorithms,
                                network embeddings, graphical models, latent variable models, matrix factorization
                                methods and more.</li>
                            <li>Dealing with the heterogeneity of the data.</li>
                            <li>The common need for information integration and alignment.</li>
                            <li>Handling dynamic and changing data.</li>
                            <li>Addressing each of these issues at scale.</li>
                        </ul>
                        <p class="large text-muted">
                            Traditionally, a number of subareas have contributed to this space: communities in graph
                            mining, learning from structured data, statistical relational learning, inductive logic
                            programming, and, moving beyond subdisciplines in computer science, social network analysis,
                            and, more broadly network science.
                        </p>
                    </div>

                    <!--div class="col-md-4 text-right">
                        <p class="large text-muted">
                            <a href="https://twitter.com/mlgworkshop?ref_src=twsrc%5Etfw" class="twitter-follow-button"
                                data-show-count="false">Follow @mlgworkshop</a>
                            <a class="twitter-timeline" data-lang="en" data-height="600"
                                data-chrome="nofooter; noheader; transparent" data-link-color="#FAB81E"
                                href="https://twitter.com/mlgworkshop?ref_src=twsrc%5Etfw"></a>
                            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                        </p>
                    </div-->

                </div>
            </div>
    </section>

    <!-- Program Section -->
    
    <section id="program" class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Schedule</h2>
                    <h3 class="section-subheading text-muted"><strong>Long Beach Convention & Entertainment Center - Grand B</strong>
                                <br />
                    </h3>
                </div>
            </div>

            <div class="row">
                <div class="col-lg-6 text-left">
                    <table class="tg">
                        <tr>
                            <th class="tg-odj0"></th>
                            <th class="tg-odj0">Morning Sessions</th>
                        </tr>
                        <tr>
                            <th class="tg-lqy6">8:00-8:15am</th>
                            <th class="tg-yw4l">
                                Opening Remarks</th>
                        </tr>
                        <tr>
                            <td class="tg-l2oz">8:15-9:00am<br />
                                <!--img src="img/speakers/jure.jpg" class="img-responsive img-circle"
                                    style="height:50px; float: right;"-->

                            </td>
                            <td class="tg-9hbo"><a class="page-scroll" href="#keynote">Keynote:</a><br />
                                Jie Tang<br />
                                Self-supervised Learning and Pre-training on Graphs
                            </td>
                        </tr>
                        <tr>

                        <tr>
                            <td class="tg-lqy6">9:00-9:30am</td>
                            <td class="tg-yw4l">Contributed Talks<br/><br/>
                                Qingkai Zeng et al.<br/>
                                Completing Taxonomies with Relation-Aware Mutual Attentions<br/><br/>
                                Rishabh Jain et al.<br/>
                                Neural Priority Queues for Graph Neural Networks (GNNs)
                                <br />
                        </tr>                            

                        <tr>
                            <td class="tg-xr8r">9:30-10:00am</td>
                            <td class="tg-kjho">Coffee Break</td>
                        </tr>

                        <tr>
                            <td class="tg-l2oz">10:00-10:45am<br />
                                <!--img src="img/speakers/danai.jpg" class="img-responsive img-circle"
                                    style="height:50px; float: right;"-->
                            </td>
                            <td class="tg-9hbo"><a class="page-scroll" href="#keynote">Keynote:</a><br />
                                Wei Wang<br />
                                TBD<br/>
                            </td>
                        </tr>
                        <tr>
                            <td class="tg-l2oz">10:45-11:30am<br />
                                <!--img src="img/speakers/jimeng_sun.jpg" class="img-responsive img-circle"
                                    style="height:50px; float: right;"-->
                            </td>
                            <td class="tg-9hbo">Panel<br />
                                Liang Zhao (Emory), Zhangyang Wang (UT Austin), Wei Cheng (NEC lab), Meng Jiang (Notre Dame)<br />
                                </td>
                        </tr>

                        <tr>
                            <td class="tg-lqy6">11:30-12:00pm</td>
                            <td class="tg-yw4l">Poster Session 1<br/>
                        </tr>                                                  

                        </tr>
                        <td class="tg-xr8r">12:00-1:00pm</td>
                        <td class="tg-kjho">Lunch Break<br /></td>
                        </tr>

                    </table>

                </div>
                <div class="col-lg-6 text-left">
                    <table class="tg">
                        <tr>
                            <th class="tg-odj0"></th>
                            <th class="tg-odj0">Afternoon Sessions</th>
                        </tr>

                        <tr>
                            <td class="tg-l2oz">1:00-1:45pm<br />
                                <!--img src="img/speakers/jimeng_sun.jpg" class="img-responsive img-circle"
                                    style="height:50px; float: right;"-->
                            </td>
                            <td class="tg-9hbo"><a class="page-scroll" href="#keynote">Keynote:</a><br />
                                Karthik Subbian<br />
                                Practical Challenges in Graph Representation Learning</td>                                
                        </tr>                        

                        <tr>
                            <td class="tg-lqy6">1:45-2:15pm</td>
                            <td class="tg-yw4l">Contributed Talks<br/><br/>
                                Prasita Mukherjee et al.<br/>
                                OCTAL: Graph Representation Learning for LTL Model Checking<br/><br/>
                                Jing Zhu et al.<br/>
                                SpotTarget: Rethinking the Effect of Target Edges for Link Prediction in GNNs
                                <br />
                        </tr>

                        <tr>
                            <td class="tg-lqy6">2:15-3:00pm</td>
                            <td class="tg-yw4l">Poster Session 2<br/>
                        </tr>                         


                        <tr>
                            <td class="tg-xr8r">3:00-3:30pm</td>
                            <td class="tg-kjho">Coffee Break</td>
                        </tr>                        

                        <tr>
                            <td class="tg-l2oz">3:30-4:15pm<br />
                                <!--img src="img/speakers/tyler.jpg" class="img-responsive img-circle"
                                    style="height:50px; float: right;"-->
                            </td>
                            <td class="tg-9hbo"><a class="page-scroll" href="#keynote">Keynote:</a><br />
                                Leman Akoglu<br />
                                Expressive, Scalable, and Interpretable Graph Embeddings</td>
                        </tr>

                        <tr>
                            <th class="tg-lqy6">4:15-4:40pm</th>
                            <th class="tg-yw4l">
                                Closing Remarks</th>
                        </tr>

                    </table>
                </div>

                <div class="col-lg-3 text-left">
                    &nbsp;
                </div>
            </div>

        </div>
    </section>

    <!-- Keynote Section no abstract -->

    <section id="keynote"  class="bg-mid">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Keynote Speakers</h2>
                </div>
            </div>

            <div class="row">

                <div class="col-md-3">
                    <div class="team-member">
                        <img src="img/speakers/jie.jpg" class="img-responsive img-circle" alt="Jie Tang" >
                        <h4>Jie Tang</h4>
                        <p class="text-muted">Professor<br/>Tsinghua University</p>
                        <ul class="list-inline social-buttons-team">
                            <li><a target=_blank href="http://keg.cs.tsinghua.edu.cn/jietang/"><i class="fa fa-home"></i></a>
                            </li>
                            <li><a target=_blank href="https://www.linkedin.com/in/jietangtsinghua/"><i class="fa fa-linkedin"></i></a>
                            </li>
                            <li><a target=_blank href="https://scholar.google.com/citations?user=n1zDCkQAAAAJ&hl=en"><i class="fa fa-graduation-cap"></i></a>
                            </li>
                        </ul>
                    </div>
                </div>

                <div class="col-md-3">
                    <div class="team-member">
                        <img src="img/speakers/wei.png" class="img-responsive img-circle" alt="Wei Wang">
                        <h4>Wei Wang</h4>
                        <p class="text-muted">Professor<br/>UCLA</p>
                        <ul class="list-inline social-buttons-team">
                            <li><a target=_blank href="https://web.cs.ucla.edu/~weiwang/"><i class="fa fa-home"></i></a>
                            </li>
                            <li><a target=_blank href="https://www.linkedin.com/in/wei-wang-8800845/"><i class="fa fa-linkedin"></i></a>
                            </li>
                            <li><a target=_blank href="https://scholar.google.com/citations?user=UedS9LQAAAAJ&hl=en"><i class="fa fa-graduation-cap"></i></a>
                            </li>
                        </ul>
                    </div>
                </div>

                <div class="col-md-3">
                    <div class="team-member">
                        <img src="img/team/Leman.jpeg" class="img-responsive img-circle" alt="Leman Akoglu">
                        <h4>Leman Akoglu</h4>
                        <p class="text-muted">Associate Professor<br />CMU</p>
                        <ul class="list-inline social-buttons-team">
                            <li><a href="https://www.andrew.cmu.edu/user/lakoglu/" target="_blank"><i class="fa fa-home"></i></a>
                            </li>
                            <li><a href="https://www.linkedin.com/in/leman-akoglu-b164329" target=_blank><i
                                        class="fa fa-linkedin"></i></a>
                            </li>
                            <li><a target=_blank
                                    href="https://scholar.google.com/citations?user=4ITkr_kAAAAJ"><i
                                        class="fa fa-graduation-cap"></i></a>
                            </li>
                        </ul>
                    </div>
                </div>

                <div class="col-md-3">
                    <div class="team-member">
                        <img src="img/speakers/karthik.jpg" class="img-responsive img-circle" alt="Karthik Subbian">
                        <h4>Karthik Subbian</h4>
                        <p class="text-muted">Senior Principal Scientist<br/>Amazon</p>
                        <ul class="list-inline social-buttons-team">
                            <li><a target=_blank href="https://sites.google.com/site/mailtosuka/"><i class="fa fa-home"></i></a>
                            </li>
                            <li><a target=_blank href="https://www.linkedin.com/in/karthik-subbian-29893095"><i class="fa fa-linkedin"></i></a>
                            </li>
                            <li><a target=_blank href="https://scholar.google.com/citations?user=6ai0lDAAAAAJ"><i class="fa fa-graduation-cap"></i></a>
                            </li>
                        </ul>
                    </div>
                </div>

            </div>            

    </section>                


    <!-- Accepted Papers Section -->
    <section id="papers" class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Accepted Papers</h2>
                    <!--h3 class="section-subheading text-muted">
                    All accepted papers will present a poster
                    </h3-->
                </div>
            </div>
            <div class="row">

                <div class="col-lg-1 text-center">
                    &nbsp;
                </div>

                <div class="col-lg-11 text-justify">
                    <!-- Begin Paper List -->

                    <p class="large text-muted">
                        <strong>Active Learning for Graphs with Noisy Structures</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid9">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib9">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_9.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Hongliang Chi, Cong Qi, Suhang Wang and Yao Ma</i><br/>
                        
                        <div id="pid9" class="collapse">
                        <strong>Abstract:</strong> Graph Neural Networks (GNNs) have seen significant success in tasks like node classification, primarily dependent on the availability of abundant labeled nodes. However, the excessive cost of labeling large-scale graphs led to a focus on active learning methods on graphs, which aim for efficient data selection. While most methods assume reliable graph topology, real-world scenarios often present noisy graphs.  Designing an active learning framework for noisy graphs is challenging, as selecting data for labeling and obtaining a clean graph are naturally interdependent: selecting high-quality data requires clean graph structure while cleaning noisy graph structure needs adequate labeled data. Considering the challenge mentioned above, we propose a robust active learning framework named GALClean that adopts an iterative approach to achieve data selection and graph purification simultaneously. Furthermore, we summarize GALClean as an instance of the Expectation-Maximization (EM) algorithm, which provides a theoretical understanding of the design and mechanisms in GALClean. Extensive experiments have demonstrated the effectiveness and robustness of our proposed method.
                        <br/><br/><strong>Keywords:</strong> Graph Neural Networks, Active Learning, Noisy Learning
                        <hr/>
                        </div>
                        
                        <div id="bib9" class="collapse">
                        @inproceedings{mlg2023_9,<br/>
                        title={Active Learning for Graphs with Noisy Structures},<br/>
                        author={Hongliang Chi, Cong Qi, Suhang Wang and Yao Ma},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>SpotTarget: Rethinking the Effect of Target Edges for Link Prediction in GNNs</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid3">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib3">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_3.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Jing Zhu, Yuhang Zhou, Vassilis Ioannidis, Shengyi Qian, Wei Ai, Xiang Song and Danai Koutra</i><br/>
                        
                        <div id="pid3" class="collapse">
                        <strong>Abstract:</strong> Graph Neural Networks (GNNs) have demonstrated promising outcomes across various tasks, including node classification and link prediction. However, despite their remarkable success in various high-impact applications, we have identified three common pitfalls in message passing for link prediction, especially within industrial settings. Particularly, in prevalent GNN frameworks (e.g., DGL and PyTorch-Geometric), the target edges (i.e., the edges being predicted) consistently exist as message passing edges in the graph during training. Consequently, this results in overfitting and distribution shift, both of which adversely impact the generalizability to test the target edges. Additionally, during test time, the failure to exclude the test target edges leads to implicit test leakage caused by neighborhood aggregation. In this paper, we analyze these three pitfalls and investigate the impact of including or excluding target edges on the performance of nodes with varying degrees during training and test phases. Our theoretical and empirical analysis demonstrates that low-degree nodes are more susceptible to these pitfalls. These pitfalls can have detrimental consequences when GNNs are implemented in production systems. To systematically address these pitfalls, we propose SppotTarget, an effective and efficient GNN training framework. During training, SpotTarget leverages our insight regarding low-degree nodes and excludes train target edges connected to at least one low-degree node. During test time, it emulates real-world scenarios of GNN usage in production and excludes all test target edges. Our experiments conducted on diverse real-world datasets, including e-commerce graphs, demonstrate that SpotTarget significantly enhances GNNs, achieving up to a 15 times increase in accuracy in sparse graphs. Furthermore, SpotTarget consistently and dramatically improves the performance of GNN models for low-degree nodes in dense graphs.
                        <br/><br/><strong>Keywords:</strong> Link Prediction, Graph Neural Network, Overfitting, Distribution Shift, Data Leakage
                        <hr/>
                        </div>
                        
                        <div id="bib3" class="collapse">
                        @inproceedings{mlg2023_3,<br/>
                        title={SpotTarget: Rethinking the Effect of Target Edges for Link Prediction in GNNs},<br/>
                        author={Jing Zhu, Yuhang Zhou, Vassilis Ioannidis, Shengyi Qian, Wei Ai, Xiang Song and Danai Koutra},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Completing Taxonomies with Relation-Aware Mutual Attentions</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid10">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib10">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_10.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Qingkai Zeng, Zhihan Zhang, Jinfeng Lin and Meng Jiang</i><br/>
                        
                        <div id="pid10" class="collapse">
                        <strong>Abstract:</strong> Taxonomies serve many applications with a structural representation of knowledge. To incorporate emerging concepts into existing taxonomies, the task of taxonomy completion aims to find suitable positions for emerging query concepts. Previous work captured homogeneous token-level interactions inside a concatenation of the query concept term and definition using pre-trained language models. However, they ignored the token-level interactions between the term and definition of the query concepts and their related concepts. In this work, we propose to capture heterogeneous token-level interactions between the different textual components of concepts that have different types of relations. We design a relation-aware mutual attention module (RAMA) to learn such interactions for taxonomy completion. Experimental results demonstrate that our new taxonomy completion framework based on RAMA achieves the state-of-the-art performance on six taxonomy datasets. This paper belongs to "Application and analysis - Knowledge Graph Construction", and in the "Novel research paper" category.
                        <br/><br/><strong>Keywords:</strong> taxonomy completion, mutual attention, concept definition, heterogeneous interactions
                        <hr/>
                        </div>
                        
                        <div id="bib10" class="collapse">
                        @inproceedings{mlg2023_10,<br/>
                        title={Completing Taxonomies with Relation-Aware Mutual Attentions},<br/>
                        author={Qingkai Zeng, Zhihan Zhang, Jinfeng Lin and Meng Jiang},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid15">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib15">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_15.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Chanakya Ekbote, Ajinkya P. Deshpande, Arun Iyer, Ramakrishna Bairi and Sundararajan Sellamanickam</i><br/>
                        
                        <div id="pid15" class="collapse">
                        <strong>Abstract:</strong> Unsupervised node representations learnt using contrastive learning-based methods have shown good performance on downstream tasks. However, these methods rely on augmentations that mimic low-pass filters, limiting their performance on tasks requiring different eigen-spectrum parts. This paper presents a simple filter-based augmentation method to capture different parts of the eigen-spectrum. We show significant improvements using these augmentations. Further, we show that sharing the same weights across these different filter augmentations is possible, reducing the computational load. In addition, previous works have shown that good performance on downstream tasks requires high dimensional representations. Working with high dimensions increases the computations, especially when multiple augmentations are involved. We mitigate this problem and recover good performance through lower dimensional embeddings using simple random Fourier feature projections. Our method, FiGURe, achieves an average gain of up to 4.4%, compared to the state-of-the-art unsupervised models, across all datasets in consideration, both homophilic and heterophilic.
                        <br/><br/><strong>Keywords:</strong> graph neural networks, contrastive learning, kernel methods
                        <hr/>
                        </div>
                        
                        <div id="bib15" class="collapse">
                        @inproceedings{mlg2023_15,<br/>
                        title={FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations},<br/>
                        author={Chanakya Ekbote, Ajinkya P. Deshpande, Arun Iyer, Ramakrishna Bairi and Sundararajan Sellamanickam},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Seq-HyGAN: Sequence Classification via Hypergraph Attention Network</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid17">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib17">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_17.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Khaled Mohammed Saifuddin, Corey May, Farhan Tanvir, Muhammad Ifte Khairul Islam and Esra Akbas</i><br/>
                        
                        <div id="pid17" class="collapse">
                        <strong>Abstract:</strong> Extracting meaningful features from sequences and devising effective similarity measures are vital for sequence data mining tasks, particularly sequence classification. While Neural Network models are commonly used to learn features of sequence automatically, they are limited to capturing adjacent structural connection information and ignore global, higher-order information between the sequences. To address these challenges, we propose a novel Hypergraph Attention Network model, namely Seq-HyGAN, for sequence classification problems. To capture the complex structural similarity between sequence data, we create a novel hypergraph model by defining higher-order relations between subsequences extracted from sequences. Subsequently, we introduce a Sequence Hypergraph Attention Network that learns sequence features by considering the significance of subsequences and sequences to one another. Through extensive experiments, we demonstrate the effectiveness of our proposed Seq-HyGAN model in accurately classifying sequence data, outperforming several state-of-the-art methods by a significant margin. This paper belongs to "Algorithms and methods- Graph neural networks and graph representation learning" and is in the "Novel research papers" category.
                        <br/><br/><strong>Keywords:</strong> Hypergraph attention network, Sequence learning, Graph learning
                        <hr/>
                        </div>
                        
                        <div id="bib17" class="collapse">
                        @inproceedings{mlg2023_17,<br/>
                        title={Seq-HyGAN: Sequence Classification via Hypergraph Attention Network},<br/>
                        author={Khaled Mohammed Saifuddin, Corey May, Farhan Tanvir, Muhammad Ifte Khairul Islam and Esra Akbas},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid29">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib29">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_29.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Ahmad Naser Eddin, Jacopo Bono, David Aparício, Hugo Ferreira, João Ascensão, Pedro Ribeiro and Pedro Bizarro</i><br/>
                        
                        <div id="pid29" class="collapse">
                        <strong>Abstract:</strong> Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks.
                        Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints, a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop operations on the incoming edges. We evaluate our proposed approach on three open-source datasets and two in-house datasets, and compare with three state-of-the-art algorithms (TGN-attn, TGN-ID, Jodie). We demonstrate that our graph-sprints features, combined with a machine learning classifier, achieve competitive performance (outperforming all baselines for the node classification tasks in five datasets). Simultaneously, graph-sprints significantly reduce inference latencies, achieving up to 9 times faster inference in our experimental setting.
                        <br/><br/><strong>Keywords:</strong> Continuous-time dynamic graphs (CTDGs), Streaming graphs, Graph representation learning, Graph feature engineering, Graph Neural Networks
                        <hr/>
                        </div>
                        
                        <div id="bib29" class="collapse">
                        @inproceedings{mlg2023_29,<br/>
                        title={From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs},<br/>
                        author={Ahmad Naser Eddin, Jacopo Bono, David Aparício, Hugo Ferreira, João Ascensão, Pedro Ribeiro and Pedro Bizarro},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>A Heterogeneous Graph-based Framework for Scalable Fraud Detection</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid4">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib4">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_4.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Phanindra Reddy Madduru and Naveed Janvekar</i><br/>
                        
                        <div id="pid4" class="collapse">
                        <strong>Abstract:</strong> The rise of online marketplaces has led to increased concerns regarding the presence of bad actors involved in counterfeit or engage in fraudulent activities. While efforts are being made by organizations to monitor and address these issues, bad actors persistently find new ways to engage in fraudulent behavior, including creating new accounts using different credentials, account hijacking etc. To combat this issue, our study proposes the use of Heterogeneous Relational Graph Convolutional Networks (HRGCN) to identify risky relationships among entities like sellers or customers. By leveraging this advanced graph-based approach, we aim to enhance the detection and mitigation of fraudulent behavior on the e-commerce marketplaces. The HRGCN model is designed to detect sellers with risky associations with other known bad sellers by analyzing various connecting edges such as encrypted device and identity credentials. With the rapid growth of e-commerce stores, the number of sellers has witnessed an exponential increase, leading to a significant expansion in their social networks formed by sharing various relationships such as digital contact information, communication channels and devices. This has made it challenging to process the data with the direct implementation of HRGCN. This highlights the importance of model scalability in handling large datasets. To address this issue, we have introduced a novel mini-batch version of HRGCN variant that works in tandem with a neighborhood sampler, which is optimized to run on GPUs, significantly reducing the training time by 70%. This mini-batch version of HRGCN maintains and/or improves the performance of the model while addressing the scalability issue, making it an efficient solution for handling large datasets. In this paper, we compare the performance of three models: a benchmark model based on Random Forest trained on seller node features alone, HRGCN trained on Full batch, and HRGCN with mini-batch implementation. The findings of our experiments reveal that the HRGCN models outperform the benchmark model with a significant improvement in both F1-score and Recall. Specifically, the HRGCN models show an impressive increase in recall by approximately 115% compared to the baseline model. Moreover, the mini-batch HRGCN model demonstrated substantial improvement in performance over the full batch HRGCN model, achieving a 16% higher F1 score and an 8% higher PR AUC score. These results emphasize the effectiveness of using a mini-batch approach to handle large datasets and detecting related bad sellers.
                        <br/><br/><strong>Keywords:</strong> Graph Neural Networks, Heterogeneous Relational Graph Convolution Networks, Neural Networks, Fraud Detection, Mini-batch, Fraudulent behavior, Risky relationships, E-commerce marketplaces, Model Scalability, Large datasets
                        <hr/>
                        </div>
                        
                        <div id="bib4" class="collapse">
                        @inproceedings{mlg2023_4,<br/>
                        title={A Heterogeneous Graph-based Framework for Scalable Fraud Detection},<br/>
                        author={Phanindra Reddy Madduru and Naveed Janvekar},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Editable Graph Neural Network for Node Classifications</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid7">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib7">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_7.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Zirui Liu, Zhimeng Jiang, Shaochen Zhong, Kaixiong Zhou, Li Li, Rui Chen, Soo-Hyun Choi and Xia Hu</i><br/>
                        
                        <div id="pid7" class="collapse">
                        <strong>Abstract:</strong> Despite Graph Neural Networks (GNNs) have achieved prominent success in many graph-based learning problems, such as credit risk assessment in financial networks and fake news detection in social networks. However, the trained GNNs still make errors and these errors may cause serious negative impact on society. \textit{Model editing}, which corrects the model behavior on wrongly predicted target samples while leaving model predictions unchanged on unrelated samples, has garnered significant interest in the fields of computer vision and natural language processing. However, model editing for graph neural networks (GNNs) is rarely explored, despite GNNs' widespread applicability. 
                        To fill the gap, we first observe that existing model editing methods significantly deteriorate prediction accuracy (up to $50\%$ accuracy drop) in GNNs while a slight accuracy drop in multi-layer perception (MLP). The rationale behind this observation is that the node aggregation in GNNs will spread the editing effect throughout the whole graph. This propagation pushes the node representation far from its original one.
                        Motivated by this observation, we propose \underline{E}ditable \underline{G}raph \underline{N}eural \underline{N}etworks (EGNN), a neighbor propagation-free approach to correct the model prediction on misclassified nodes. Specifically, EGNN simply stitches an MLP to the underlying GNNs, where the weights of GNNs are frozen during model editing. In this way, EGNN disables the propagation during editing while still utilizing the neighbor propagation scheme for node prediction to obtain satisfactory results. 
                        Experiments demonstrate that EGNN outperforms existing baselines in terms of effectiveness (correcting wrong predictions with lower accuracy drop), generalizability (correcting wrong predictions for other similar nodes), and efficiency (low training time and memory) on various graph datasets.
                        <br/><br/><strong>Keywords:</strong> Graph neural networks, Editable training, Node Classification
                        <hr/>
                        </div>
                        
                        <div id="bib7" class="collapse">
                        @inproceedings{mlg2023_7,<br/>
                        title={Editable Graph Neural Network for Node Classifications},<br/>
                        author={Zirui Liu, Zhimeng Jiang, Shaochen Zhong, Kaixiong Zhou, Li Li, Rui Chen, Soo-Hyun Choi and Xia Hu},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Topological Representation Learning for E-commerce Shopping Behaviors</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid13">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib13">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_13.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Yankai Chen, Quoc-Tuan Truong, Xin Shen, Ming Wang, Jin Li, Jim Chan and Irwin King</i><br/>
                        
                        <div id="pid13" class="collapse">
                        <strong>Abstract:</strong> Learning compact representation from customer shopping behaviors is at the core of web-scale E-commerce recommender systems. At Amazon, we put great efforts into learning embedding of customer engagements in order to fuel multiple downstream tasks for better recommendation services. In this work, we define the notion of shopping trajectory that consists of customer interactions at the categorical level of products, then construct an end-to-end model namely C-STAR which is capable of learning rich embedding for representing the variable-length customer trajectory. C-STAR explicitly captures the inter-trajectory distribution similarity and intra-trajectory semantic correlation, providing a coarse-to-fine trajectory representation learning paradigm both structurally and semantically. We evaluate the model on Amazon proprietary data as well as four public datasets, where the learned embeddings have shown to be effective for customer-centric tasks including customer segmentation and shopping trajectory completion. These tasks empower multiple personalized shopping experiences for our customers. This paper belongs to "Application and analysis: Large-scale analysis and modeling", and in the “Novel research papers” category.
                        <br/><br/><strong>Keywords:</strong> Topological Representation Learning, Shopping Trajectory, Amazon Recommendation
                        <hr/>
                        </div>
                        
                        <div id="bib13" class="collapse">
                        @inproceedings{mlg2023_13,<br/>
                        title={Topological Representation Learning for E-commerce Shopping Behaviors},<br/>
                        author={Yankai Chen, Quoc-Tuan Truong, Xin Shen, Ming Wang, Jin Li, Jim Chan and Irwin King},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid16">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib16">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_16.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Sitan Yang, Malcolm Wolff, Shankar Ramasubramanian, Ronak Mehta and Michael Mahoney</i><br/>
                        
                        <div id="pid16" class="collapse">
                        <strong>Abstract:</strong> Encoder-decoder deep neural networks have been increasingly studied for multi-horizon time series forecasting especially in real-world applications. However, these sophisticated neural forecasters typically rely on a large number of time series examples with substantial history to forecast accurately. A rapidly growing topic of interest is forecasting time series which lack sufficient historical data---often referred to as the ``cold start'' problem. In this paper, we introduce a novel yet simple method to address this problem by leveraging graph neural networks (GNNs) as a data augmentation for enhancing the encoder used by such forecasters. These GNN-based features can capture complex inter-series relationships and their generation process is optimized end-to-end with the forecasting task. We show that our architecture can use either data-driven or domain knowledge defined graphs, scaling to jointly incorporate information from multiple very large graphs with millions of nodes. In our target application of demand forecasting for a large e-commerce retailer, we demonstrate on both a small dataset of 100K products and a large dataset with over 2 million products that our method improves overall performance over competitive baseline models. More importantly, we show that it brings substantially more gains to ``cold start'' products such as those newly launched or recently out-of-stock.
                        <br/><br/><strong>Keywords:</strong> Time Series, Multi-Horizon Forecasting, Graph Neural Networks, Data Augmentation
                        <hr/>
                        </div>
                        
                        <div id="bib16" class="collapse">
                        @inproceedings{mlg2023_16,<br/>
                        title={GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting},<br/>
                        author={Sitan Yang, Malcolm Wolff, Shankar Ramasubramanian, Ronak Mehta and Michael Mahoney},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Fair Online Dating Recommendations for Sexually Fluid Users via Leveraging Opposite Gender Interaction Ratio</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid22">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib22">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_22.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Yuying Zhao, Yu Wang, Yi Zhang, Pamela Wisniewski, Charu Aggarwal and Tyler Derr</i><br/>
                        
                        <div id="pid22" class="collapse">
                        <strong>Abstract:</strong> Novel research paper:
                        Online dating platforms have gained widespread popularity as a means for individuals to seek potential romantic relationships. While recommender systems have been designed to improve the user experience in dating platforms by providing personalized recommendations, increasing concerns about fairness have encouraged the development of fairness-aware recommender systems from various perspectives (e.g., gender and race). However, sexual orientation, which plays a significant role in finding a satisfying relationship, is under-investigated. To fill this crucial gap, we propose a novel metric, Opposite Gender Interaction Ratio (OGIR), as a way to investigate potential unfairness for users with varying/fluid preferences towards the opposite gender. We empirically analyze a real online dating dataset and observe existing recommender algorithms could suffer from group unfairness according to OGIR. We further investigate the potential causes for such gaps in recommendation quality, which lead to the challenges of group data imbalance and group calibration imbalance. Ultimately, we propose a fair recommender system based on re-weighting and re-ranking strategies to respectively mitigate these associated imbalance challenges. Experimental results demonstrate both strategies improve fairness while their combination achieves the best performance towards maintaining model utility while improving fairness.
                        <br/><br/><strong>Keywords:</strong> Fair Recommendations, Online Dating Networks, Social Network Analysis
                        <hr/>
                        </div>
                        
                        <div id="bib22" class="collapse">
                        @inproceedings{mlg2023_22,<br/>
                        title={Fair Online Dating Recommendations for Sexually Fluid Users via Leveraging Opposite Gender Interaction Ratio},<br/>
                        author={Yuying Zhao, Yu Wang, Yi Zhang, Pamela Wisniewski, Charu Aggarwal and Tyler Derr},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Compact Interpretable Tensor Graph Multi-Modal News  Embeddings</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid24">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib24">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_24.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Dawon Ahn, William Shiao, Andrew Bauer, Arindam Khaled, Stefanos Poulis and Evangelos Papalexakis</i><br/>
                        
                        <div id="pid24" class="collapse">
                        <strong>Abstract:</strong> Online news articles encompass a variety of modalities such as text and images. How can we learn a representation that incorporates information from all those modalities in a compact and interpretable manner, while also being useful in a variety of downstream tasks? Recent advances in Large Language and Vision Models have made it possible to represent image and text data as embeddings, which can then be used to perform downstream tasks. Despite these developments, these embedding models tend to generate high-dimensional embeddings, making them problematic in terms of compactness and interpretability. 
                        
                        In this paper, we propose CITEM (Compact Interpretable Tensor graph multi-modal news EMbedding), which is a novel framework for multi-modal news representations via tensor decomposition in a compact and interpretable way. CITEM generates a tensor graph consisting of a news similarity graph for each modality and employs a tensor decomposition to produce compact and interpretable embeddings, each dimension of which is a heterogeneous co-cluster of news articles and corresponding modalities. Interpretability and compactness are key, since our proposed embeddings contain few dimensions which lend themselves to inspection and explanation. Traditional tensor analysis has so far been restricted to transductive learning scenarios (e.g., in the form of semi-supervised learning), but CITEM includes two variants for inductive learning, which essentially enables us to represent unseen news articles. We extensively validate CITEM compared to baselines on two news classification tasks: misinformation news detection and news categorization. The experimental results show that CITEM performs within the same range of AUC as state-of-the-art baselines while producing 7× to 10.5× more compact embeddings. In addition, each embedding dimension of CITEM is interpretable, representing a latent co-cluster of articles.
                        <br/><br/><strong>Keywords:</strong> Tensor decomposition, Multi-modal tensor graph, Interpretable news embeddings
                        <hr/>
                        </div>
                        
                        <div id="bib24" class="collapse">
                        @inproceedings{mlg2023_24,<br/>
                        title={Compact Interpretable Tensor Graph Multi-Modal News  Embeddings},<br/>
                        author={Dawon Ahn, William Shiao, Andrew Bauer, Arindam Khaled, Stefanos Poulis and Evangelos Papalexakis},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Spectral Clustering of Attributed Multi-relational Graphs*</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid27">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib27">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_27.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Ylli Sadikaj, Yllka Velaj, Sahar Behzadi and Claudia Plant</i><br/>
                        
                        <div id="pid27" class="collapse">
                        <strong>Abstract:</strong> Graph clustering aims at discovering a natural grouping of the nodes such that similar nodes are assigned to a common cluster. Many different algorithms have been proposed in the literature: for simple graphs, for graphs with attributes associated to nodes, and for graphs where edges represent different types of relations among nodes. However, complex data in many domains can be represented as both attributed and multi-relational networks.
                        In this paper, we propose SpectralMix, a joint dimensionality reduction technique for multi-relational graphs with categorical node attributes. SpectralMix integrates all information available from the attributes, the different types of relations, and the graph structure to enable a sound interpretation of the clustering results. Moreover, it generalizes existing techniques: it reduces to spectral embedding and clustering when only applied to a single graph and to homogeneity analysis when applied to categorical data. 
                        Experiments conducted on several real-world datasets enable us to detect dependencies between graph structure and categorical attributes, moreover, they exhibit the superiority of SpectralMix over existing methods.
                        The full version of this paper has also appeared in the Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
                        Data Mining, KDD’21, and received the Student Best Paper Award.
                        
                        ∗This paper won the Student Best Paper Award at KDD ’21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery Data Mining, August 2021, Pages 1431–1440, https://doi.org/10.1145/3447548.34673
                        <br/><br/><strong>Keywords:</strong> Graph embedding, Spectral clustering, Multi-relational graphs, Attributed graphs
                        <hr/>
                        </div>
                        
                        <div id="bib27" class="collapse">
                        @inproceedings{mlg2023_27,<br/>
                        title={Spectral Clustering of Attributed Multi-relational Graphs*},<br/>
                        author={Ylli Sadikaj, Yllka Velaj, Sahar Behzadi and Claudia Plant},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Data Sampling using Locality Sensitive Hashing for Large Scale Graph Learning</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid2">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib2">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_2.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Sarath Shekkizhar, Neslihan Bulut, Mohamed Farghal, Sasan Tavakkol, Mohammadhossein Bateni and Animesh Nandi</i><br/>
                        
                        <div id="pid2" class="collapse">
                        <strong>Abstract:</strong> An important step in graph-based data analysis and processing is the construction of similarity graphs. Recent works, such as [7, 23 ], have focused on the semi-supervised setting to learn an optimal similarity function for constructing a task-optimal graph. However, in many scenarios with billions of data points and trillions of potential edges, the run-time and computational requirements for training the similarity model make these approaches impractical. In this work, we consider data sampling as a means to overcome this issue. Unlike typical sampling use-cases which only seek diversity, the similarity-learning for graph construction problem requires data samples that are both diverse and representative of highly similar data points. We present an efficient sampling approach by taking an adaptive partition view of locality sensitive hashing. Theoretically, we show that, though the samples obtained are correlated with sampling probabilities that do not sum to one, the training loss estimated for learning the graph similarity model using our approach is unbiased with a smaller variance compared to random sampling. Experiments on public datasets demonstrate the superior generalization of similarity models learned via our sampling. In a real large-scale industrial abuse-detection example, we observe ≈10× increase in identifying abusive items while having a lower false positive rate compared to the baseline.
                        <br/><br/><strong>Keywords:</strong> Sampling, Graph learning, Locality sensitive hashing
                        <hr/>
                        </div>
                        
                        <div id="bib2" class="collapse">
                        @inproceedings{mlg2023_2,<br/>
                        title={Data Sampling using Locality Sensitive Hashing for Large Scale Graph Learning},<br/>
                        author={Sarath Shekkizhar, Neslihan Bulut, Mohamed Farghal, Sasan Tavakkol, Mohammadhossein Bateni and Animesh Nandi},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Multi-Task Learning on Heterogeneous Graph Neural Network for Substitute Recommendation</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid6">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib6">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_6.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Tianchen Zhou, Michinari Momma, Chaosheng Dong, Fan Yang, Chenghuan Guo, Jin Shang and Jia Liu</i><br/>
                        
                        <div id="pid6" class="collapse">
                        <strong>Abstract:</strong> Substitute recommendation in e-commerce has attracted increasing 
                        attention in recent years, to help improve customer experience. In 
                        this work, we propose a multi-task graph learning framework that 
                        jointly learns from supervised and unsupervised objectives with 
                        heterogeneous graphs. Particularly, we propose a new contrastive 
                        method that extracts global information from both positive and 
                        negative neighbors. By feeding substitute signal data from different 
                        sources to learning tasks with different focuses, our model learns 
                        the representation of products that can be applied for substitute 
                        identification under different substitutable criteria. We conduct ex- 
                        periments on Amazon datasets, and the experiment results demon- 
                        strate that our method outperforms all existing baselines in terms 
                        of comprehensive performance among all metrics of interest.
                        <br/><br/><strong>Keywords:</strong> Online shopping, Recommendation, Graph Neural Network
                        <hr/>
                        </div>
                        
                        <div id="bib6" class="collapse">
                        @inproceedings{mlg2023_6,<br/>
                        title={Multi-Task Learning on Heterogeneous Graph Neural Network for Substitute Recommendation},<br/>
                        author={Tianchen Zhou, Michinari Momma, Chaosheng Dong, Fan Yang, Chenghuan Guo, Jin Shang and Jia Liu},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid14">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib14">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_14.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Mohammad Ali Alomrani, Mahdi Biparva, Yingxue Zhang and Mark Coates</i><br/>
                        
                        <div id="pid14" class="collapse">
                        <strong>Abstract:</strong> Temporal graph neural networks have shown promising results in learning inductive representations by automatically extracting temporal patterns. However, previous works often rely on complex memory modules or inefficient random walk methods to construct temporal representations. In addition, the existing dynamic graph encoders are non-trivial to adapt to self-supervised paradigms, which prevents them from utilizing unlabeled data. To address these limitations, we present an efficient yet effective attention-based encoder that leverages temporal edge encodings and window-based subgraph sampling to generate task-agnostic embeddings. Moreover, we propose a joint-embedding architecture using non-contrastive SSL to learn rich temporal embeddings without labels. Experimental results on 7 benchmark datasets indicate that on average, our model outperforms SoTA baselines on the future link prediction task by 4.23% for the transductive setting and 3.30% for the inductive setting while only requiring 5-10x less training/inference time. Additionally, we empirically validate the SSL pre-training significance under two probings commonly used in language and vision modalities. Lastly, different aspects of the proposed framework are investigated through experimental analysis and ablation studies.
                        <br/><br/><strong>Keywords:</strong> dynamic graphs, graph neural networks, self-supervised learning
                        <hr/>
                        </div>
                        
                        <div id="bib14" class="collapse">
                        @inproceedings{mlg2023_14,<br/>
                        title={DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision},<br/>
                        author={Mohammad Ali Alomrani, Mahdi Biparva, Yingxue Zhang and Mark Coates},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>A Large Scale Synthetic Graph Dataset Generation Framework</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid19">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib19">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_19.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Sajad Darabi, Piotr Bigaj, Dawid Majchrowski, Artur Kasymov, Pawel Morkisz and Alex Fit-Florea</i><br/>
                        
                        <div id="pid19" class="collapse">
                        <strong>Abstract:</strong> Recently there has been increasing interest in developing and deploying deep graph learning algorithms for many graph analysis tasks such as node and edge classification, link prediction, and clustering with numerous practical applications such as fraud detection, drug discovery, or recommender systems. Albeit there is a limited number of publicly available graph-structured datasets, most of which are tiny compared to production-sized applications with trillions of edges and billions of nodes or are limited in their application domain.
                        In this work, we tackle this shortcoming by proposing a scalable synthetic graph generation tool. This tool can be used to learn a set of parametric models from proprietary datasets that can subsequently be released to researchers to study various graph methods on the synthetic data increasing prototype development and novel applications. Finally, the performance of the graph learning algorithms depends not only on the size but also on the graph datasets structure. We show how our framework generalizes across a set of datasets, mimicking both structural and feature distributions and the ability to scale them across varying dataset sizes. Code can be found at \href{https://github.com/}{https://github.com}
                        <br/><br/><strong>Keywords:</strong> Large Scale Dataset, Graphs, Synthetic Data, Generative Modeling
                        <hr/>
                        </div>
                        
                        <div id="bib19" class="collapse">
                        @inproceedings{mlg2023_19,<br/>
                        title={A Large Scale Synthetic Graph Dataset Generation Framework},<br/>
                        author={Sajad Darabi, Piotr Bigaj, Dawid Majchrowski, Artur Kasymov, Pawel Morkisz and Alex Fit-Florea},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>GraphBoost: Adaptive Boosting Node Generation for Class-Imbalanced Graphs</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid11">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib11">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_11.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Yuhe Gao, Sheng Zhang and Rui Song</i><br/>
                        
                        <div id="pid11" class="collapse">
                        <strong>Abstract:</strong> Classification in imbalanced data, where the majority class has a much larger representation than the minority class, has been a significant topic in recent decades.  Two popular approaches for handling this issue are (1) rebalancing the sizes of classes through reweighting, resampling, or synthetic nodes generating, and (2) focusing on the data points that are hard to classify to enhance the classifier performance. In graphical data, several methods, such as GraphSMOTE and GraphENS, from the first type have been developed recently for class-imbalanced node classification tasks, but few adaptations of the second approach have been proposed. In response to this gap , we present a novel multi-stage boosting framework inspired by the second approach. In particular, the framework proposed in this research paper jointly generates the topological structure and features of synthetic nodes by minimizing the distance of synthetic nodes and misclassified nodes from previous training stages.  Our experiments on class-imbalanced graphs show that our novel framework outperforms standard graph neural networks. Furthermore, our framework can be combined with existing methods (such as GraphENS) resulting in further performance enhancements.
                        <br/><br/><strong>Keywords:</strong> Imbalanced Graph, Graph Neural Network, Generative Model, Node Classification
                        <hr/>
                        </div>
                        
                        <div id="bib11" class="collapse">
                        @inproceedings{mlg2023_11,<br/>
                        title={GraphBoost: Adaptive Boosting Node Generation for Class-Imbalanced Graphs},<br/>
                        author={Yuhe Gao, Sheng Zhang and Rui Song},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Effect of Deception in Influence Maximization and Polarization on Social Networks: A Sheaf Laplacian Approach</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid23">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib23">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_23.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Mehmet Aktas, Esra Akbas, Ashley Hahn and Mehmet Ahsen</i><br/>
                        
                        <div id="pid23" class="collapse">
                        <strong>Abstract:</strong> In the contemporary era of social media and online communication, comprehending the dynamics of information diffusion in social networks has become crucial. This research article investigates the effects of deception on information diffusion, specifically focusing on influence maximization and polarization in social networks. We propose an analytic model of deception within social networks. Building upon the sheaf Laplacian diffusion model derived from algebraic topology, we examine opinion dynamics in the presence of deception. Next, we redefine the Laplacian centrality, an influential node detection method originally designed for regular graphs, to quantify the influence of deception in influence maximization using the sheaf Laplacian. Additionally, we employ the sheaf Laplacian to model polarization in networks and investigate the impact of deception on polarization using two distinct polarization measures. Through extensive experiments conducted on synthetic and real-world networks, our findings suggest that deceptive individuals wield more influence than honest users within social networks. Furthermore, we demonstrate that deception amplifies polarization in networks, with influential individuals playing a significant role in deepening the polarization phenomenon.
                        <br/><br/><strong>Keywords:</strong> information diffusion, sheaf Laplacian, influence maximization, polarization, deception
                        <hr/>
                        </div>
                        
                        <div id="bib23" class="collapse">
                        @inproceedings{mlg2023_23,<br/>
                        title={Effect of Deception in Influence Maximization and Polarization on Social Networks: A Sheaf Laplacian Approach},<br/>
                        author={Mehmet Aktas, Esra Akbas, Ashley Hahn and Mehmet Ahsen},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Semi-Supervised Embedding of Attributed Multiplex Networks*</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid26">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib26">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_26.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Ylli Sadikaj, Justus Rass, Yllka Velaj and Claudia Plant</i><br/>
                        
                        <div id="pid26" class="collapse">
                        <strong>Abstract:</strong> Complex information can be represented as networks (graphs) characterized by a large number of nodes, multiple types of nodes, and multiple types of relationships between them, i.e. multiplex networks. Additionally, these networks are enriched with different types of node features.
                        We propose a Semi-supervised Embedding approach for Attributed Multiplex Networks (SSAMN), to jointly embed nodes, node attributes, and node labels of multiplex networks in a low dimensional space. Network embedding techniques have garnered research attention for real-world applications. However, most existing techniques solely focus on learning the node embeddings, and only a few learn class label embeddings. Our method assumes that we have different classes of nodes and that we know the class label of some, very few nodes for every class. Guided by this type of supervision, SSAMN learns a low-dimensional representation incorporating all information in a large labeled multiplex network. SSAMN integrates techniques from Spectral Embedding and Homogeneity Analysis to improve the embedding of nodes, node attributes, and node labels. Our experiments demonstrate that we only need very few labels per class in order to have a final embedding that preserves the information of the graph. To evaluate the performance of SSAMN, we run experiments on four real-world datasets. The results show that our approach outperforms state-of-the-art methods for downstream tasks such as semi-supervised node classification and node
                        clustering.
                        This paper has also appeared in the Proceedings of the ACM
                        Web Conference 2023, former WWW.
                        
                        ∗This paper has appeared in WWW ’23: Proceedings of the ACM Web Conference
                        2023, April 2023, Pages 578–587, https://doi.org/10.1145/3543507.3583
                        <br/><br/><strong>Keywords:</strong> Network Embedding, Multiplex Networks, Attributed Networks
                        <hr/>
                        </div>
                        
                        <div id="bib26" class="collapse">
                        @inproceedings{mlg2023_26,<br/>
                        title={Semi-Supervised Embedding of Attributed Multiplex Networks*},<br/>
                        author={Ylli Sadikaj, Justus Rass, Yllka Velaj and Claudia Plant},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>UGGS: A Unified Graph Generation Framework Based on Self-Supervised Learning</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid20">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib20">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_20.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Sajad Ramezani and Soroor Motie</i><br/>
                        
                        <div id="pid20" class="collapse">
                        <strong>Abstract:</strong> Deep learning on graphs has gained interest in recent years. The applicability of graphs to model problems in various domains, such as chemical molecules, financial transactions, parse trees, etc., has encouraged researchers to develop and extend machine learning methods from other data modalities, such as text and image, to graphs. Generative models have been used extensively in recent years and have achieved significant milestones, especially in text and image generation. However, graph generative models have not been developed as extensively, and fundamental problems are still in the discussion phase. This work addresses some of these problems, such as the lack of an integrated framework and interpretable evaluation metric, by introducing a unified framework for the graph generation task. The base of the proposed framework is on the appropriate graph and node embeddings to estimate graphs' distribution. Hence it composes of graph neural networks to embed the nodes and graphs and also enhances the quality of graph embeddings via the introduction of pseudo tasks in a self-supervised fashion. Self-supervised techniques have proven useful in enhancing generative models to be more robust and generalizable. This work proposes several pseudo tasks and evaluates their performance on common graph datasets. It also emphasizes the problem of graph decoding and speculates that graph generation strategy matters, and one can establish more complex graph generation models to generate higher-quality graphs. It also proposes a distance metric in embedding space for generated graphs to filter out poorly generated data. In the end, the proposed framework achieves competitive results compared to previously proposed models while having fewer parameters and a shorter training time. We have also made our framework implementation available.
                        
                        <br/><br/><strong>Keywords:</strong> graph generative models, graph neural network, self-supervised learning, generative models, machine learning on graphs
                        <hr/>
                        </div>
                        
                        <div id="bib20" class="collapse">
                        @inproceedings{mlg2023_20,<br/>
                        title={UGGS: A Unified Graph Generation Framework Based on Self-Supervised Learning},<br/>
                        author={Sajad Ramezani and Soroor Motie},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Graph Model Explainer Tool</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid5">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib5">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_5.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Yudi Zhang, Phanindra Reddy Madduru, Naveed Janvekar and Nitika Bhaskar</i><br/>
                        
                        <div id="pid5" class="collapse">
                        <strong>Abstract:</strong> Graph Neural Networks (GNNs) have gained popularity in various fields, such as recommendation systems, social network analysis and fraud detection. However, despite their effectiveness, the topological nature of GNNs makes it challenging for users to understand the model predictions. To address this challenge, we built a user-friendly UI to visualize the most important relationships for both homogeneous and heterogeneous static graphs models, which a post-hoc explanation technique called GNNExplainer is implemented. This UI can be applied to a wide range of applications that use graph models. It offers an intuitive and interpretable way for users to understand the complex relationships within a graph and how they influence the model's predictions.
                        <br/><br/><strong>Keywords:</strong> GNN, GNNExplainer, Visualization
                        <hr/>
                        </div>
                        
                        <div id="bib5" class="collapse">
                        @inproceedings{mlg2023_5,<br/>
                        title={Graph Model Explainer Tool},<br/>
                        author={Yudi Zhang, Phanindra Reddy Madduru, Naveed Janvekar and Nitika Bhaskar},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>
                        
                        <p class="large text-muted">
                        <strong>Computation of Node Distances on Hypergraphs</strong> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#pid8">Abstract</button> 
                        <button class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bib8">BibTex</button> 
                        <a href="papers/MLG__KDD_2023_paper_8.pdf" target=_blank class="btn btn-primary btn-xs" role="button">PDF</a> 
                        <br/>
                        <i>Enzhi Li and Bilal Fadlallah</i><br/>
                        
                        <div id="pid8" class="collapse">
                        <strong>Abstract:</strong> A hypergraph is a generalization of a graph that arises naturally when we consider attribute-sharing among entities. Although a hypergraph can be converted into a graph by expanding its hyperedges into fully connected subgraphs, going the reverse way is computationally complex and NP-complete. We hence hypothesize that a hypergraph contains more information than a graph. Moreover, it is more convenient to manipulate a hypergraph directly, rather than expanding it into a graph. An open problem in hypergraphs is how to accurately and efficiently calculate their node distances. Once node distances are defined, we can find a node's nearest neighbors, and perform label propagation on hypergraphs using a K-nearest neighbors (KNN) approach. In this paper, we propose two methods to achieve this. In the first, we compute expected hitting times of random walks on a hypergraph as node distances; in the second, we generalize the DeepWalk method to hypergraphs. We note that simple random walks (SRW) cannot accurately describe highly complex real-world hypergraphs, which motivates us to introduce frustrated random walks (FRW) to better describe them. Using real-world datasets, we show that FRW and DeepWalk can beat SRW with a large margin. For large and sparse hypergraphs, our method for computing the expected hitting times of random walks is approximately linear in time complexity, rendering it superior to the DeepWalk method.
                        <br/><br/><strong>Keywords:</strong> Graph algorithm, Stochastic process, Machine learning algorithm, Applied mathematics, KNN algorithm
                        <hr/>
                        </div>
                        
                        <div id="bib8" class="collapse">
                        @inproceedings{mlg2023_8,<br/>
                        title={Computation of Node Distances on Hypergraphs},<br/>
                        author={Enzhi Li and Bilal Fadlallah},<br/>
                        booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},<br/>
                        year={2023}<br/>
                        }
                        <hr/>
                        </div>
                        
                        </p>                                               
                    
                    <!-- End Paper List -->
                </div>
            </div>
        </div>
    </section>                      

    <!-- Call for Papers Section -->
    <section id="call">
        <!--class="bg-mid-gray"-->
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Call for Papers</h2>
                    <!--h3 class="section-subheading text-muted">Will be announced soon!</h3-->
                </div>
                <div class="row text-justify">
                    <div class="col-md-12">
                        <p class="large text-muted">
                            This workshop is a forum for exchanging ideas and methods for mining and learning with graphs, developing new common understandings of the problems at hand, sharing of data sets where applicable, and leveraging existing knowledge from different disciplines. The goal is to bring together researchers from academia, industry, and government, to create a forum for discussing recent advances in graph analysis. In doing so, we aim to better understand the overarching principles and the limitations of our current methods and to inspire research on new algorithms and techniques for mining and learning with graphs.
                        </p>
                        <p class="large text-muted">
                            To reflect the broad scope of work on mining and learning with graphs, we encourage submissions that span the spectrum from theoretical analysis to algorithms and implementation, to applications, empirical studies and reflection papers. As an example, the growth of user-generated content on blogs, microblogs, discussion forums, product reviews, etc., has given rise to a host of new opportunities for graph mining in the analysis of social media. More recently, the advent of neural methods for learning graph representations has spurred numerous works in embedding network entities for diverse applications including ranking and retrieval, traffic routing and drug-discovery.  We encourage submissions on theory, methods, and applications focusing on a broad range of graph-based approaches in various domains.
                        </p>
                        <p class="large text-muted">
                            Topics of interest include, but are not limited to:
                        </p>

                        <ul class="large text-muted">
                            <li><b>Theoretical aspects:</b>
                                <ul class="large text-muted">
                                    <li>Computational or statistical learning theory related to graphs</li>
                                    <li>Theoretical analysis of graph algorithms or models</li>
                                    <li>Sampling and evaluation issues in graph algorithms</li>
                                    <li>Analysis of dynamic graphs</li>
                                </ul>
                            </li>
                            <li><b>Algorithms and methods:</b>
                                <ul class="large text-muted">
                                    <li>Graph mining</li>
                                    <li>Probabilistic and graphical models for structured data</li>
                                    <li>Heterogeneous/multi-model graph analysis</li>
                                    <li>Graph neural networks and graph representation learning</li>
                                    <li>Statistical models of graph structure</li>
                                    <li>Combinatorial graph methods</li>
                                    <li>Semi-supervised learning, active learning, transductive inference, and transfer
                                        learning in the context of graph</li>
                                </ul>
                            </li>
                            <li><b>Applications and analysis:</b>
                                <ul class="large text-muted">
                                    <li>Analysis of social media</li>
                                    <li>Analysis of biological networks</li>
                                    <li>Knowledge graph construction</li>
                                    <li>Large-scale analysis and modeling</li>
                                </ul>
                            </li>
                        </ul>

                        <p class="large text-muted">
                            We welcome many kinds of papers, such as, but not limited to:
                        </p>

                        <ul class="large text-muted">
                            <li>Novel research papers
                            </li>
                            <li>Demo papers
                            </li>
                            <li>Work-in-progress papers
                            </li>
                            <li>Visionary papers (white papers)
                            </li>
                            <li>Appraisal papers of existing methods and tools (e.g., lessons learned)
                            </li>
                            <li>Evaluatory papers which revisit validity of domain assumptions
                            </li>
                            <li>Relevant work that has been previously published
                            </li>
                            <li>Work that will be presented at the main conference
                            </li>
                        </ul>

                        <p class="large text-muted">
                            Authors should <strong>clearly indicate</strong> in their abstracts the kinds of submissions
                            that the papers belong to, to help reviewers better understand their contributions. <br />
                            All papers will be peer reviewed, single-blinded.
                            Submissions must be in PDF, <strong>no more than 8 pages long</strong> — shorter papers are
                            welcome — and formatted according to the standard double-column <a
                                href="http://www.acm.org/publications/proceedings-template#aL2" target=_blank>ACM
                                Proceedings Style</a>. <br />
                            The accepted papers will be published on the workshop’s website and will not be considered
                            archival for resubmission purposes. <br />
                            Authors whose papers are accepted to the workshop will have the opportunity to participate
                            in a spotlight and poster session, and some set will also be chosen for oral presentation.

                        </p>

                        <p class="large text-muted">
                            <strong>For paper submission, please proceed to the <a
                                    href="https://easychair.org/conferences/?conf=mlgkdd2023" target=_blank>submission
                                    website</a>.</strong>
                        </p>

                        <p class="large text-muted">
                            <strong>Please send enquiries to chair@mlgworkshop.org.</strong>
                        </p>

                        <p class="large text-muted">
                            To receive updates about the current and future workshops and the Graph Mining community,
                            please join the <a href="https://groups.google.com/d/forum/mlg-list" target=_blank>Mailing
                                List</a>, or follow the <a href="https://twitter.com/mlgworkshop" target=_blank>Twitter
                                Account</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Dates Section -->
    <section id="dates" class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Important Dates</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 text-left">
                    &nbsp;
                </div>
                <div class="col-lg-6 text-left">
                    <div class="col-md-12">
                        <p class="large text-muted">
                            <b>Paper Submission Deadline:</b> May 30, 2023
                        </p>
                        <p class="large text-muted">
                            <b>Author Notification:</b> June 23, 2023
                        </p>
                        <p class="large text-muted">
                            <b>Camera Ready:</b> July 10, 2023
                        </p>
                        <p class="large text-muted">
                            <b>Workshop:</b> August 7, 2023
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Organization Section -->
    <section id="organization">
        <!--class="bg-mid-gray"-->
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Workshop Organizers</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row">
                <div class="col-sm-1">
                </div>
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/team/neil.jpg" class="img-responsive img-circle" alt="Neil Shah">
                        <h4>Neil Shah</h4>
                        <p class="text-muted">Lead Research Scientist<br />Snap Inc.</p>
                        <ul class="list-inline social-buttons-team">
                            <li><a href="http://nshah.net/" target="_blank"><i
                                        class="fa fa-home"></i></a>
                            </li>
                            <li><a class="inactive" href="#organization"><i
                                        class="fa fa-twitter"></i></a>
                            </li>
                            <li><a href="https://www.linkedin.com/in/nshah171/" target=_blank><i
                                        class="fa fa-linkedin"></i></a>
                            </li>
                            <li><a target=_blank href="https://scholar.google.com/citations?user=Qut69OgAAAAJ&hl=en"><i
                                        class="fa fa-graduation-cap"></i></a>
                            </li>
                        </ul>
                    </div>
                </div>
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/team/shobeir.jpg" class="img-responsive img-circle" alt="Shobeir Fakhraei">
                        <h4>Shobeir Fakhraei</h4>
                        <p class="text-muted">Senior Applied Scientist<br />Amazon</p>
                        <ul class="list-inline social-buttons-team">
                            <li><a href="http://www.cs.umd.edu/~shobeir/" target="_blank"><i class="fa fa-home"></i></a>
                            </li>
                            <li><a href="https://twitter.com/shobeirf" target=_blank><i class="fa fa-twitter"></i></a>
                            </li>
                            <li><a href="http://www.linkedin.com/in/shobeir" target=_blank><i
                                        class="fa fa-linkedin"></i></a>
                            </li>
                            <li><a target=_blank href="https://scholar.google.com/citations?user=6vJwj_QAAAAJ"><i
                                        class="fa fa-graduation-cap"></i></a>
                            </li>
                        </ul>
                    </div>
                </div>   
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/team/da.jpg" class="img-responsive img-circle" alt="Da Zheng">
                        <h4>Da Zheng</h4>
                        <p class="text-muted">Senior Applied Scientist<br />Amazon</p>
                        <ul class="list-inline social-buttons-team">
                            <li><a href="https://zheng-da.github.io/" target="_blank"><i
                                        class="fa fa-home"></i></a>
                            </li>
                            <li><a href="https://twitter.com/zhengda1936" target=_blank><i class="fa fa-twitter"></i></a>
                            </li>
                            <li><a href="https://www.linkedin.com/in/da-zheng-22604b13/" target=_blank><i
                                        class="fa fa-linkedin"></i></a>
                            </li>
                            <li><a target=_blank href="https://scholar.google.ca/citations?hl=en&user=b1PYJN0AAAAJ"><i
                                        class="fa fa-graduation-cap"></i></a>
                            </li>
                        </ul>
                    </div>
                </div>
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/team/bahare.jpg" class="img-responsive img-circle" alt="Bahare Fatemi">
                        <h4>Bahare Fatemi</h4>
                        <p class="text-muted">Research Scientist<br />Google Research</p>
                        <ul class="list-inline social-buttons-team">
                            <li><a href="https://baharefatemi.github.io/homepage/" target="_blank"><i class="fa fa-home"></i></a>
                            </li>
                            <li><a href="https://twitter.com/BahareFatemi" target=_blank><i class="fa fa-twitter"></i></a>
                            </li>
                            <li><a href="https://www.linkedin.com/in/bahare-fatemi-b0049179/" target=_blank><i
                                        class="fa fa-linkedin"></i></a>
                            </li>
                            <li><a target=_blank
                                    href="https://scholar.google.ca/citations?user=ddybJHUAAAAJ&hl=en"><i
                                        class="fa fa-graduation-cap"></i></a>
                            </li>
                        </ul>
                    </div>
                </div>
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/team/Leman.jpeg" class="img-responsive img-circle" alt="Leman Akoglu">
                        <h4>Leman Akoglu</h4>
                        <p class="text-muted">Associate Professor<br />CMU</p>
                        <ul class="list-inline social-buttons-team">
                            <li><a href="https://www.andrew.cmu.edu/user/lakoglu/" target="_blank"><i class="fa fa-home"></i></a>
                            </li>
                            <li><a href="https://twitter.com/leman_akoglu" target=_blank><i class="fa fa-twitter"></i></a>
                            </li>
                            <li><a href="https://www.linkedin.com/in/leman-akoglu-b164329" target=_blank><i
                                        class="fa fa-linkedin"></i></a>
                            </li>
                            <li><a target=_blank
                                    href="https://scholar.google.com/citations?user=4ITkr_kAAAAJ"><i
                                        class="fa fa-graduation-cap"></i></a>
                            </li>
                        </ul>
                    </div>
                </div>
                <div class="col-sm-1">
                </div>
                </div>
            </div>
            <div class="row" style="margin-top:60px;">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Program Committee</h2>
                    <div class="col-md-2">
                        &nbsp;
                    </div>
                    <div class="col-md-5 text-left">
                        <p class="large text-muted">
                            Anton Tsitsulin (Google)<br />
                            Evangelos Papalexakis (University of California Riverside)<br />
                            Elena Zheleva (University of Illinois at Chicago)<br />
                            Yozen Liu (University of Southern California)<br />
                            Mohammad Hasan (IUPUI)<br />
                            Stefan Wrobel (Fraunhofer IAIS & Univ. of Bonn)<br />
                            Jan Ramon (INRIA)<br />
                            David Gleich (Purdue University)<br />
                            John Palowitch (Google Research)<br />
                            Sami Abu-El-Haija (Google)<br />
                        </p>
                    </div>
                    <div class="col-md-5 text-left">
                        <p class="large text-muted">
                            Ali Pinar (Sandia National Laboratories)<br />
                            Puja Trivedi (University of Michigan)<br />
                            Hocine Cherifi (University of Burgundy)<br />
                            Boris Knyazev (Samsung)<br />
                            Tong Zhao (Snap Inc.)<br />
                            Aris Anagnostopoulos (Sapienza University of Rome)<br />
                            Zhongfei Zhang (SUNY Binghamton)<br />
                            Shichang Zhang (University of California, Los Angeles)<br />
                            Perouz Taslakian (McGill University)<br />
                            Ivan Brugere (University of Illinois at Chicago)<br />
                            William Shiao (UC Riverside)<br />
                        </p>    
                    </div>
                    <div class="col-md-1">
                        &nbsp;
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- History Section -->
    <section id="history" class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Previous Workshops</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 text-left">
                    &nbsp;
                </div>

                <div class="col-lg-6 text-left">
                    <p class="large text-muted">
                        <a href="http://www.mlgworkshop.org/2022/" target=_blank class="large text-muted">2022,
                            Washington, DC, USA (co-located with KDD)</a></br>                        
                        <a href="https://mlg-europe.github.io/2022/" target=_blank class="large text-muted">2022,
                            Grenoble, France (co-located with ECML-PKDD)</a></br>
                        <a href="http://www.mlgworkshop.org/2020/" target=_blank class="large text-muted">2020,
                            Virtual (co-located with KDD)</a></br>
                        <a href="http://www.mlgworkshop.org/2019/" target=_blank class="large text-muted">2019,
                            Anchorage, USA (co-located with KDD)</a></br>
                        <a href="http://www.mlgworkshop.org/2018/" target=_blank class="large text-muted">2018, London,
                            United Kingdom (co-located with KDD)</a></br>
                        <a href="http://www.mlgworkshop.org/2017/" target=_blank class="large text-muted">2017, Halifax,
                            Nova Scotia, Canada (co-located with KDD)</a></br>
                        <a href="http://www.mlgworkshop.org/2016/" target=_blank class="large text-muted">2016, San
                            Francisco, USA (co-located with KDD)</a></br>
                        <a href="http://snap.stanford.edu/mlg2013/" target=_blank class="large text-muted">2013,
                            Chicago, USA (co-located with KDD)</a></br>
                        <a href="http://dtai.cs.kuleuven.be/events/mlg2012/" target=_blank
                            class="large text-muted">2012, Edinburgh, Scotland (co-located with ICML)</a></br>
                        <a href="http://www.cs.purdue.edu/mlg2011/" target=_blank class="large text-muted">2011, San
                            Diego, USA (co-located with KDD)</a></br>
                        <a href="http://www.cs.umd.edu/mlg2010/" target=_blank class="large text-muted">2010,
                            Washington, USA (co-located with KDD)</a></br>
                        <a href="http://dtai.cs.kuleuven.be/ilp-mlg-srl//" target=_blank class="large text-muted">2009,
                            Leuven, Belgium (co-located with SRL and ILP)</a></br>
                        <a href="http://research.ics.aalto.fi/events/MLG08/" target=_blank
                            class="large text-muted">2008, Helsinki, Finland (co-located with ICML)</a></br>
                        <a href="http://mlg07.dsi.unifi.it/" target=_blank class="large text-muted">2007, Firenze,
                            Italy</a></br>
                        <a href="http://www.inf.uni-konstanz.de/mlg2006/index.shtml" target=_blank
                            class="large text-muted">2006, Berlin, German (co-located with ECML-PKDD)</a></br>
                        <a href="#" class="large text-muted">2005, Porto, Portugal, October 7, 2005</a></br>
                        <a href="http://hms.liacs.nl/mgts2004/" target=_blank class="large text-muted">2004, Pisa,
                            Italy, September 24, 2004</a></br>
                        <a href="http://www.ar.sanken.osaka-u.ac.jp/MGTS-2003CFP.html" target=_blank
                            class="large text-muted">2003, Cavtat-Dubrovnik, Croatia</a></br>
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="bg-darkest-gray">
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <span class="copyright" style="color:gray;">Copyright &copy; MLG Workshop 2023</span>
                </div>
                <div class="col-md-4">
                    <ul class="list-inline social-buttons">
                        <li><a href="https://twitter.com/mlgworkshop" target=_blank><i class="fa fa-twitter"></i></a>
                        </li>
                        <!--li><a href="#"><i class="fa fa-facebook"></i></a>
                        </li>
                        <li><a href="#"><i class="fa fa-linkedin"></i></a>
                        </li-->
                    </ul>
                </div>
                <!--div class="col-md-4">
                    <ul class="list-inline quicklinks">
                        <li><a href="#">Privacy Policy</a>
                        </li>
                        <li><a href="#">Terms of Use</a>
                        </li>
                    </ul>
                </div-->
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="http://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="js/classie.js"></script>
    <script src="js/cbpAnimatedHeader.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/agency.js"></script>

</body>

</html>
